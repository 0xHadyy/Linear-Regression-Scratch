# 📊 Linear Regression From Scratch (Math + Python)

This project implements Multiple Linear Regression from scratch using only NumPy. It covers both the closed-form Ordinary Least Squares (OLS) solution and Gradient Descent optimization, with full mathematical derivations, visualizations, and supporting PDF documents.

---

## Topics Covered

- Mean Squared Error (MSE)
- Ordinary Least Squares (OLS) Estimation
-  Gradient Descent Optimization
- 📉 Loss Function: Derivation and Implementation
- 🎯 Coefficient Accuracy & Inference:
  - Standard Error of β̂
  - Confidence Intervals
  - T-tests, F-tests
- 📊 Model Accuracy:
  - Residual Standard Error (RSE)
  - R-Squared (R²)
  - Prediction Intervals
- 🔍 Assumptions & Diagnostics:
  - Linearity
  - Independence of Errors
  - Homoscedasticity / Heteroscedasticity
  - Residual Analysis & Plots
- ⚠️ Challenges in Linear Regression:
  - Outliers
  - High Leverage Points
  - Multicollinearity


## 💻 How To Run

```bash
git clone https://github.com/0xHadyy/Linear-Regression-From-Scratch
cd Linear-Regression-From-Scratch

# Run the notebook
jupyter notebook notebook.ipynb

# Or run the script
python main.py
```
## 📚 References

- An Introduction to Statistical Learning — James, Witten, Hastie, Tibshirani

- The Elements of Statistical Learning — Hastie, Tibshirani, Friedman

- My own derivations, notes, and code

- GitHub Repo: https://github.com/0xHadyy/Linear-Regression-From-Scratch

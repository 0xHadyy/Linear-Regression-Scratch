# ğŸ“Š Linear Regression From Scratch (Math + Python)

This project implements Multiple Linear Regression from scratch using only NumPy. It covers both the closed-form Ordinary Least Squares (OLS) solution and Gradient Descent optimization, with full mathematical derivations, visualizations, and supporting PDF documents.

---

## Topics Covered

- Mean Squared Error (MSE)
- Ordinary Least Squares (OLS) Estimation
-  Gradient Descent Optimization
- ğŸ“‰ Loss Function: Derivation and Implementation
- ğŸ¯ Coefficient Accuracy & Inference:
  - Standard Error of Î²Ì‚
  - Confidence Intervals
  - T-tests, F-tests
- ğŸ“Š Model Accuracy:
  - Residual Standard Error (RSE)
  - R-Squared (RÂ²)
  - Prediction Intervals
- ğŸ” Assumptions & Diagnostics:
  - Linearity
  - Independence of Errors
  - Homoscedasticity / Heteroscedasticity
  - Residual Analysis & Plots
- âš ï¸ Challenges in Linear Regression:
  - Outliers
  - High Leverage Points
  - Multicollinearity


## ğŸ’» How To Run

```bash
git clone https://github.com/0xHadyy/Linear-Regression-From-Scratch
cd Linear-Regression-From-Scratch

# Run the notebook
jupyter notebook notebook.ipynb

# Or run the script
python main.py
```
## ğŸ“š References

- An Introduction to Statistical Learning â€” James, Witten, Hastie, Tibshirani

- The Elements of Statistical Learning â€” Hastie, Tibshirani, Friedman

- My own derivations, notes, and code

- GitHub Repo: https://github.com/0xHadyy/Linear-Regression-From-Scratch
